{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word_embedding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP6y0WIOGdOLey0k/JI1Dbr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ogigo/text-classification/blob/main/word_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wPGaXcFM7TV8"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle"
      ],
      "metadata": {
        "id": "v2eDxovY7dTI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_token = {\"username\":\"shahedshoyab\",\"key\":\"1aa2d9748604f09744dac84eb27b107f\"}\n",
        "\n",
        "import json\n",
        "\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as file:json.dump(api_token, file)"
      ],
      "metadata": {
        "id": "eAntCpKs7fbn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "EQhN3GGX7j7G"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download clmentbisaillon/fake-and-real-news-dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRFQz6Ka7lMe",
        "outputId": "7b068d54-268c-4da1-d66f-159a9838ada0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading fake-and-real-news-dataset.zip to /content\n",
            " 93% 38.0M/41.0M [00:00<00:00, 222MB/s] \n",
            "100% 41.0M/41.0M [00:00<00:00, 205MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip fake-and-real-news-dataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmC3YriK7lGG",
        "outputId": "892e129a-b25b-4d66-e96b-d5d5ece666cd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  fake-and-real-news-dataset.zip\n",
            "  inflating: Fake.csv                \n",
            "  inflating: True.csv                \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "syzLGXBT8xQO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true = pd.read_csv(\"/content/True.csv\")\n",
        "fake = pd.read_csv(\"/content/Fake.csv\")"
      ],
      "metadata": {
        "id": "Gx1XXOFA8euO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing unnecessary colums\n",
        "true = true.drop(['subject', 'date', 'title'], axis=1)\n",
        "fake = fake.drop(['subject', 'date', 'title'], axis=1)"
      ],
      "metadata": {
        "id": "u-FBxXJL8en_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# labelling the data\n",
        "true['label'] = 1\n",
        "fake['label'] = 0"
      ],
      "metadata": {
        "id": "wj9snd778el3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# concatenating both dataframes\n",
        "df = pd.concat([true, fake])\n",
        "# shuffling the data\n",
        "df = df.sample(frac = 1)\n",
        "# resetting the index values\n",
        "df.reset_index(inplace=True, drop=True)"
      ],
      "metadata": {
        "id": "RTFdbQob8ejP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing duplicate values\n",
        "df.drop_duplicates(subset=None, keep='first', inplace=True)\n",
        "df.reset_index(inplace=True, drop=True)"
      ],
      "metadata": {
        "id": "A0aO14xY8ege"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "NTUueh918edW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from bs4 import BeautifulSoup\n",
        "import re,string,unicodedata\n",
        "from keras.preprocessing import text, sequence\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from string import punctuation\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Embedding,LSTM,Dropout\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "68eWJWoNATT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop = set(stopwords.words('english'))\n",
        "punctuation = list(string.punctuation)\n",
        "stop.update(punctuation)"
      ],
      "metadata": {
        "id": "cA-0SwJuAhc1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "#Removing the square brackets\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "# Removing URL's\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub(r'http\\S+', '', text)\n",
        "#Removing the stopwords from text\n",
        "def remove_stopwords(text):\n",
        "    final_text = []\n",
        "    for i in text.split():\n",
        "        if i.strip().lower() not in stop:\n",
        "            final_text.append(i.strip())\n",
        "    return \" \".join(final_text)\n",
        "#Removing the noisy text\n",
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    text = remove_stopwords(text)\n",
        "    return text\n",
        "#Apply function on review column\n",
        "df['text']=df['text'].apply(denoise_text)"
      ],
      "metadata": {
        "id": "FomsmuIt867_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "t-G3sVMoAZ3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(df.text,df.label,random_state = 0)"
      ],
      "metadata": {
        "id": "2JhLnulsA2vN"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_features=10000\n",
        "max_len=300"
      ],
      "metadata": {
        "id": "uqiqMAndBCM9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=text.Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(x_train)"
      ],
      "metadata": {
        "id": "EAf3So33BLgM"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_train=tokenizer.texts_to_sequences(x_train)\n",
        "tokenizer_test=tokenizer.texts_to_sequences(x_test)"
      ],
      "metadata": {
        "id": "RHLC2USKB91l"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer_train)"
      ],
      "metadata": {
        "id": "aEt7geeBPy_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train=sequence.pad_sequences(tokenizer_train,maxlen=max_len)\n",
        "x_test=sequence.pad_sequences(tokenizer_test,maxlen=max_len)"
      ],
      "metadata": {
        "id": "wR4j6f5_CmoG"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "L-IiaON8Mklg"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "metadata": {
        "id": "ZsJYkeU1Mkkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_glove_file = os.path.join(\"/content/glove.6B.100d.txt\")\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuL-87vXKVmp",
        "outputId": "6a7242d2-f511-472b-8635-56946a51ec30"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_index=tokenizer.word_index\n",
        "number_words=len(word_index)+2\n",
        "embedding_dim=100\n",
        "hits=0\n",
        "misses=0"
      ],
      "metadata": {
        "id": "g9hGqM5OS2eu"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix=np.zeros((number_words,embedding_dim))\n",
        "for word,i in word_index.items():\n",
        "  embedding_vector=embeddings_index.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i]=embedding_vector\n",
        "    hits=hits+1\n",
        "  else:\n",
        "    misses=misses+1\n",
        "\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jE9inGak2QDm",
        "outputId": "e6159dda-aa6c-4c5b-94b7-839da229ce07"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 70537 words (44745 misses)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix"
      ],
      "metadata": {
        "id": "o8J5mosHzaiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding=Embedding(\n",
        "    input_dim=number_words,\n",
        "    output_dim=embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.constant(embedding_matrix),\n",
        "    trainable=False\n",
        ")"
      ],
      "metadata": {
        "id": "km85JoTx6Akd"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining Neural Network\n",
        "model = Sequential()\n",
        "#Non-trainable embeddidng layer\n",
        "model.add(embedding)\n",
        "#LSTM \n",
        "model.add(LSTM(units=128 , return_sequences = True , recurrent_dropout = 0.25 , dropout = 0.25))\n",
        "model.add(LSTM(units=64 , recurrent_dropout = 0.1 , dropout = 0.1))\n",
        "model.add(Dense(units = 32 , activation = 'relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr = 0.01), loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cp1DbOxT_Ekl",
        "outputId": "6a6d86ed-c65f-4d19-ec2a-e73997d47971"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy',\n",
        "                                            patience = 2, \n",
        "                                            verbose=1,\n",
        "                                            factor=0.5,\n",
        "                                            min_lr=0.00001)"
      ],
      "metadata": {
        "id": "b1IdVKSPD2ww"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size = 256 , \n",
        "                    validation_data = (x_test,y_test) , \n",
        "                    epochs = 1 , \n",
        "                    callbacks = [learning_rate_reduction])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2H6qA-NfEERV",
        "outputId": "d30956aa-5dd3-4292-82fe-3fa6d8c26029"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "114/114 [==============================] - 578s 5s/step - loss: 0.1873 - accuracy: 0.9201 - val_loss: 0.1071 - val_accuracy: 0.9591 - lr: 0.0100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6jmPaxv4EXXG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}